# rag_pipeline.py

import os
from dotenv import load_dotenv
from typing import List, Dict

# GEREKLÄ° KÃœTÃœPHANELER
from datasets import load_dataset
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# Ortam deÄŸiÅŸkenlerini (.env) yÃ¼kle
load_dotenv()

# API anahtarÄ±nÄ± kontrol et ve gerekirse dÃ¼zenle
if "GEMINI_API_KEY" in os.environ and "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = os.environ["GEMINI_API_KEY"]

if "GOOGLE_API_KEY" not in os.environ:
    print("HATA: GOOGLE_API_KEY veya GEMINI_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±.")
    exit()

# --- A. SABÄ°TLER VE KONFÄ°GÃœRASYON ---

CHROMA_DB_PATH = "chroma_db"

# Embedding Model SeÃ§imi (Yerel model ile API limiti aÅŸÄ±lÄ±r)
USE_LOCAL_EMBEDDINGS = True

if USE_LOCAL_EMBEDDINGS:

    EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2" # Yerel (HuggingFace) model
else:

    EMBEDDING_MODEL = "models/embedding-001" # Google Gemini API modeli

LLM_MODEL = "gemini-2.5-flash" # Cevap Ã¼retimi iÃ§in hÄ±zlÄ± Gemini modeli



# RAG'Ä±n bilgi Ã§ekeceÄŸi iki ana kategori (Ã‡ok KatmanlÄ± YapÄ±)

DATASET_CONFIG = {
    "kodlama_rehberi": {
        "datasets": [
            "TokenBender/code_instructions_122k_alpaca_style",
            "b-mc2/sql-create-context",
            "red1xe/code_instructions"
        ],
        "text_column": "instruction",  # Metin sÃ¼tunu adÄ±
        "split_percentage": "3%"
    },
    "hata_ayiklama_asistani": {
        "datasets": [
            "devangb4/scikit-learn-issues"
        ],
        "text_column": "text",         # Sorun aÃ§Ä±klamalarÄ±nÄ±n sÃ¼tunu
        "split_percentage": "15%"
    }
}

# --- B. VERÄ° Ä°ÅLEME VE Ä°NDEKSLEME FONKSÄ°YONLARI ---

def load_and_split_data(dataset_names: List[str], text_column: str, split_percentage: str) -> List[Document]:
    """
    Hugging Face veri setlerini yÃ¼kler, parÃ§alara ayÄ±rÄ±r ve LangChain Document objelerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

    """
    all_chunks = []

    for ds_name in dataset_names:
        print(f"-> Veri Seti YÃ¼kleniyor: {ds_name} ({split_percentage})")

         # Veri seti split kontrolÃ¼: train, all ve test split'lerini dener
        try:
            dataset = load_dataset(ds_name, split=f'train[:{split_percentage}]')
        except Exception as e:
            # train yoksa, 'all' split'ini deniyoruz
            print(f"  âš  'train' split bulunamadÄ±, 'all' deneniyor...")
            try:
                dataset = load_dataset(ds_name, split=f'all[:{split_percentage}]')
            except Exception as e2:
                # 'all' da yoksa, 'test' split'ini deniyoruz
                print(f"  âš  'all' split bulunamadÄ±, 'test' deneniyor...")
                try:
                    dataset = load_dataset(ds_name, split=f'test[:{split_percentage}]')
                except Exception as e3:
                    # HiÃ§bir split bulunamazsa hata verir ve sonraki veri setine geÃ§er
                    print(f"  âŒ Veri seti yÃ¼klenemedi. Bilinen split'ler bulunamadÄ±.")
                    continue  # DÃ¶ngÃ¼de sonraki veri setine geÃ§er

        # Ä°lgili metin sÃ¼tununu kullanarak LangChain Document objelerine dÃ¶nÃ¼ÅŸtÃ¼r
        documents = [
            Document(page_content=str(row[text_column]), metadata={"source_dataset": ds_name})
            for row in dataset if text_column in row and row[text_column] is not None
        ]

        print(f"  âœ“ {len(documents)} dokÃ¼man yÃ¼klendi")

        # ParÃ§alama (Chunking)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )

        chunks = text_splitter.split_documents(documents)
        all_chunks.extend(chunks)

    print(f"âœ“ Toplam {len(all_chunks)} parÃ§a oluÅŸturuldu.")
    return all_chunks



def setup_vector_stores() -> Dict[str, Chroma]:
    """
    Ã‡ok katmanlÄ± RAG mimarisi iÃ§in VektÃ¶r Ä°ndekslerini (ChromaDB) kurar veya yÃ¼kler.

    """
    if USE_LOCAL_EMBEDDINGS:
        print("\nğŸ“¥ Yerel embedding modeli kullanÄ±lÄ±yor...")
        print("âš ï¸  Ä°lk Ã§alÄ±ÅŸtÄ±rmada ~1.5GB model indirilecek (sonraki Ã§alÄ±ÅŸtÄ±rmalarda anÄ±nda hazÄ±r)")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… Yerel embedding modeli hazÄ±r!\n")
        batch_size = 500
    else:
        print("\nğŸŒ Google Gemini embeddings kullanÄ±lÄ±yor...")
        embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
        print("âœ… Gemini embeddings hazÄ±r!\n")
        batch_size = 100

    vector_stores = {}

    for topic, config in DATASET_CONFIG.items():
        db_path = os.path.join(CHROMA_DB_PATH, topic)

        # Ä°ndeks yoksa oluÅŸtur, varsa yÃ¼kle
        if not os.path.exists(db_path) or not os.listdir(db_path):
            print(f"\n--- {topic.upper()} Ä°ndeksi YENÄ°DEN OluÅŸturuluyor ---")

            # Veri yÃ¼kleme ve parÃ§alama
            chunks = load_and_split_data(
                config["datasets"],
                config["text_column"],
                config["split_percentage"]
            )

            if not chunks:
                print(f"  âš  {topic} iÃ§in hiÃ§ chunk oluÅŸturulamadÄ±, atlanÄ±yor...")
                continue

            # ChromaDB'ye Batch Halinde Ä°ndeksleme
            BATCH_SIZE = batch_size
            print(f"  ğŸ“¦ {len(chunks)} chunk, {BATCH_SIZE}'lÃ¼k batch'ler halinde iÅŸleniyor...")

            vectorstore = None
            for i in range(0, len(chunks), BATCH_SIZE):
                batch = chunks[i:i+BATCH_SIZE]
                batch_num = (i // BATCH_SIZE) + 1
                total_batches = (len(chunks) + BATCH_SIZE - 1) // BATCH_SIZE

                print(f"  â³ Batch {batch_num}/{total_batches} iÅŸleniyor... ({len(batch)} chunk)")

                if vectorstore is None:
                    # Ä°lk batch: Yeni vectorstore oluÅŸtur
                    vectorstore = Chroma.from_documents(
                        documents=batch,
                        embedding=embeddings,
                        persist_directory=db_path
                    )
                else:
                    # Sonraki batch'ler: Mevcut vectorstore'a ekle
                    vectorstore.add_documents(batch)


            print(f"âœ“ {topic.upper()} Ä°ndeksi BaÅŸarÄ±yla OluÅŸturuldu ({db_path})")
        else:
            print(f"\n--- {topic.upper()} Ä°ndeksi YÃ¼kleniyor ---")
            # Ã–nceden oluÅŸturulmuÅŸ DB'yi yÃ¼kle
            vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
            print(f"âœ“ {topic.upper()} Ä°ndeksi YÃ¼klendi")

        vector_stores[topic] = vectorstore

    return vector_stores

# --- C. RAG SÄ°STEMÄ°NÄ°N BAÅLATILMASI ---

def initialize_rag_system():
    """
    RAG sistemini baÅŸlatÄ±r, VektÃ¶r DepolarÄ±nÄ± yÃ¼kler ve LLM'i hazÄ±rlar.
    Bu, app.py tarafÄ±ndan Ã§aÄŸrÄ±lÄ±r.
    """
    print("\nğŸš€ Ã‡ok KatmanlÄ± RAG Sistemi BaÅŸlatÄ±lÄ±yor...\n")

    # AdÄ±m 1: VektÃ¶r Ä°ndekslerini Kurma/YÃ¼kleme
    vector_stores = setup_vector_stores()

    if not vector_stores:
        raise Exception("HiÃ§bir vektÃ¶r deposu oluÅŸturulamadÄ±!")

    # AdÄ±m 2: LLM'i HazÄ±rlama (Gemini)
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2)

    # VarsayÄ±lan retriever (routing mantÄ±ÄŸÄ± app.py'de yer alÄ±r)
    default_topic = list(vector_stores.keys())[0]
    default_retriever = vector_stores[default_topic].as_retriever(search_kwargs={"k": 4})

    default_rag_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=default_retriever,
        return_source_documents=True  # Kaynak gÃ¶sterimi iÃ§in
    )

    return default_rag_chain, vector_stores, llm


# --- D. UYGULAMA MANTIÄI ---

if __name__ == "__main__":
    try:
        # Ä°ndeksleme iÅŸlemini tetikler
        rag_chain, vector_stores, llm = initialize_rag_system()

        print("\n" + "="*60)
        print("âœ… TÃœM ChromaDB Ä°NDEKSLERÄ° BAÅARIYLA HAZIRLANDI")
        print("="*60)
        print(f"\nHazÄ±r VektÃ¶r DepolarÄ±: {list(vector_stores.keys())}")
        print("\nğŸ’¡ ArtÄ±k Streamlit arayÃ¼zÃ¼nÃ¼ (app.py) Ã§alÄ±ÅŸtÄ±rabilirsiniz!")

        # Test sorgusu (opsiyonel)
        print("\n--- Test Sorgusu ---")
        test_result = rag_chain({"query": "What is scikit-learn?"})
        print(f"Soru: What is scikit-learn?")
        print(f"Cevap: {test_result['result'][:200]}...")

    except Exception as e:
        print(f"\nâŒ KRÄ°TÄ°K HATA: RAG Sistemi baÅŸlatÄ±lamadÄ±.")
        print(f"Hata DetayÄ±: {e}")
        print("\nğŸ” Kontrol Listesi:")
        print("  1. GEMINI_API_KEY .env dosyasÄ±nda tanÄ±mlÄ± mÄ±?")
        print("  2. Sanal ortam (venv) aktif mi?")
        print("  3. Ä°nternet baÄŸlantÄ±nÄ±z Ã§alÄ±ÅŸÄ±yor mu?")
        print("  4. requirements.txt dosyasÄ± kuruldu mu? (pip install -r requirements.txt)")
