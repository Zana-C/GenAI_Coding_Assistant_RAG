# rag_pipeline.py

import os
from dotenv import load_dotenv
from typing import List, Dict

# GEREKLÄ° KÃœTÃœPHANELER
from datasets import load_dataset
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# Ortam deÄŸiÅŸkenlerini (.env) yÃ¼kle
load_dotenv()

# API anahtarÄ±nÄ± kontrol et ve gerekirse dÃ¼zenle
if "GEMINI_API_KEY" in os.environ and "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = os.environ["GEMINI_API_KEY"]

if "GOOGLE_API_KEY" not in os.environ:
    print("HATA: GOOGLE_API_KEY veya GEMINI_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±.")
    exit()

# --- A. SABÄ°TLER VE KONFÄ°GÃœRASYON ---

CHROMA_DB_PATH = "chroma_db"

# Embedding Model SeÃ§imi (Yerel model ile API limiti aÅŸÄ±lÄ±r)
USE_LOCAL_EMBEDDINGS = True

if USE_LOCAL_EMBEDDINGS:

    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" # Yerel (HuggingFace) model
else:

    EMBEDDING_MODEL = "models/embedding-001" # Google Gemini API modeli

LLM_MODEL = "gemini-2.0-flash" # Cevap Ã¼retimi iÃ§in hÄ±zlÄ± Gemini modeli



# RAG'Ä±n bilgi Ã§ekeceÄŸi iki ana kategori (Ã‡ok KatmanlÄ± YapÄ±)

DATASET_CONFIG = {
    "kodlama_rehberi": {
        "datasets": [
            {"name": "TokenBender/code_instructions_122k_alpaca_style", "column": "instruction"},
            {"name": "b-mc2/sql-create-context", "column": "question"},
            {"name": "red1xe/code_instructions", "column": "instruction"}
        ],
        "split_percentage": "3%"
    },
    "hata_ayiklama_asistani": {
        "datasets": [
            {"name": "devangb4/scikit-learn-issues", "column": "text"}
        ],
        "split_percentage": "15%"
    }
}

# --- B. VERÄ° Ä°ÅLEME VE Ä°NDEKSLEME FONKSÄ°YONLARI ---

def load_and_split_data(datasets_info: List[Dict], split_percentage: str) -> List[Document]:
    """
    TR: Hugging Face veri setlerini yÃ¼kler, yapÄ±landÄ±rÄ±lmÄ±ÅŸ sÃ¼tunlara gÃ¶re dÃ¶kÃ¼mana Ã§evirir ve parÃ§alar.
    EN: Loads Hugging Face datasets, converts them into documents based on structured columns, and chunks them.
    """
    all_chunks = []

    # TR: Liste iÃ§indeki her bir veri seti konfigÃ¼rasyonu iÃ§in dÃ¶ngÃ¼ baÅŸlat
    # EN: Start a loop for each dataset configuration in the list
    for ds_info in datasets_info:
        # TR: SÃ¶zlÃ¼kten veri seti adÄ±nÄ± ve ilgili metin sÃ¼tununu al
        # EN: Retrieve the dataset name and the relevant text column from the dictionary
        ds_name = ds_info["name"]
        text_column = ds_info["column"] 
        
        print(f"-> Veri Seti YÃ¼kleniyor / Loading Dataset: {ds_name} ({split_percentage})")

        # TR: Veri setini farklÄ± split (ayrÄ±m) adlarÄ±na gÃ¶re yÃ¼klemeyi dene
        # EN: Attempt to load the dataset using different split names
        try:
            # TR: Ã–ncelikle 'train' (eÄŸitim) setini belirlenen yÃ¼zde kadar yÃ¼kle
            # EN: First, try loading the 'train' split with the specified percentage
            dataset = load_dataset(ds_name, split=f'train[:{split_percentage}]')
        except Exception:
            try:
                # TR: 'train' yoksa, tÃ¼m veriyi kapsayan 'all' split'ini dene
                # EN: If 'train' is missing, try the 'all' split
                dataset = load_dataset(ds_name, split=f'all[:{split_percentage}]')
            except Exception:
                # TR: HiÃ§bir split bulunamazsa hata mesajÄ± ver ve bir sonrakine geÃ§
                # EN: If no split is found, print error and continue to the next dataset
                print(f"  âŒ {ds_name} yÃ¼klenemedi / failed to load.")
                continue

        # TR: Ham veriyi LangChain 'Document' formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
        # EN: Transform raw data into LangChain 'Document' format
        documents = [
            # TR: Metni string'e Ã§evir ve kaynaÄŸÄ± metadata (Ã¼stveri) olarak ekle
            # EN: Convert text to string and add the source as metadata
            Document(page_content=str(row[text_column]), metadata={"source_dataset": ds_name})
            for row in dataset 
            # TR: SÃ¼tun kontrolÃ¼ ve boÅŸ veri (None) kontrolÃ¼ yaparak hatalarÄ± Ã¶nle
            # EN: Prevent errors by checking for column existence and None values
            if text_column in row and row[text_column] is not None
        ]

        print(f"  âœ“ {len(documents)} dokÃ¼man yÃ¼klendi / docs loaded (SÃ¼tun/Column: {text_column})")

        # TR: BÃ¼yÃ¼k metinleri anlamlÄ± parÃ§alara ayÄ±rmak iÃ§in splitter (ayÄ±rÄ±cÄ±) yapÄ±landÄ±r
        # EN: Configure the splitter to break down large texts into meaningful chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,        # TR: Her parÃ§anÄ±n karakter sÄ±nÄ±rÄ± / EN: Character limit per chunk
            chunk_overlap=200,      # TR: ParÃ§alar arasÄ± baÄŸlamsal Ã§akÄ±ÅŸma / EN: Contextual overlap between chunks
            separators=["\n\n", "\n", " ", ""] # TR: AyÄ±rma Ã¶ncelikleri / EN: Splitting priorities
        )
        
        # TR: DokÃ¼manlarÄ± parÃ§ala ve ana listeye ekle
        # EN: Split documents and extend the main list
        chunks = text_splitter.split_documents(documents)
        all_chunks.extend(chunks)

    # TR: OluÅŸturulan tÃ¼m parÃ§alarÄ± (chunk) dÃ¶ndÃ¼r
    # EN: Return all generated chunks
    return all_chunks



def setup_vector_stores() -> Dict[str, Chroma]:
    """
    Ã‡ok katmanlÄ± RAG mimarisi iÃ§in VektÃ¶r Ä°ndekslerini (ChromaDB) kurar veya yÃ¼kler.

    """
    if USE_LOCAL_EMBEDDINGS:
        print("\nğŸ“¥ Yerel embedding modeli kullanÄ±lÄ±yor...")

        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… Yerel embedding modeli hazÄ±r!\n")
        batch_size = 500
    else:
        print("\nğŸŒ Google Gemini embeddings kullanÄ±lÄ±yor...")
        embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
        print("âœ… Gemini embeddings hazÄ±r!\n")
        batch_size = 100

    vector_stores = {}

    for topic, config in DATASET_CONFIG.items():
        db_path = os.path.join(CHROMA_DB_PATH, topic)

        # Ä°ndeks yoksa oluÅŸtur, varsa yÃ¼kle
        if not os.path.exists(db_path) or not os.listdir(db_path):
            print(f"\n--- {topic.upper()} Ä°ndeksi YENÄ°DEN OluÅŸturuluyor ---")

            # Veri yÃ¼kleme ve parÃ§alama
            chunks = load_and_split_data(
                config["datasets"],
                config["split_percentage"]
            )

            if not chunks:
                print(f"  âš  {topic} iÃ§in hiÃ§ chunk oluÅŸturulamadÄ±, atlanÄ±yor...")
                continue

            # ChromaDB'ye Batch Halinde Ä°ndeksleme
            BATCH_SIZE = batch_size
            print(f"  ğŸ“¦ {len(chunks)} chunk, {BATCH_SIZE}'lÃ¼k batch'ler halinde iÅŸleniyor...")

            vectorstore = None
            for i in range(0, len(chunks), BATCH_SIZE):
                batch = chunks[i:i+BATCH_SIZE]
                batch_num = (i // BATCH_SIZE) + 1
                total_batches = (len(chunks) + BATCH_SIZE - 1) // BATCH_SIZE

                print(f"  â³ Batch {batch_num}/{total_batches} iÅŸleniyor... ({len(batch)} chunk)")

                if vectorstore is None:
                    # Ä°lk batch: Yeni vectorstore oluÅŸtur
                    vectorstore = Chroma.from_documents(
                        documents=batch,
                        embedding=embeddings,
                        persist_directory=db_path
                    )
                else:
                    # Sonraki batch'ler: Mevcut vectorstore'a ekle
                    vectorstore.add_documents(batch)


            print(f"âœ“ {topic.upper()} Ä°ndeksi BaÅŸarÄ±yla OluÅŸturuldu ({db_path})")
        else:
            print(f"\n--- {topic.upper()} Ä°ndeksi YÃ¼kleniyor ---")
            # Ã–nceden oluÅŸturulmuÅŸ DB'yi yÃ¼kle
            vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
            print(f"âœ“ {topic.upper()} Ä°ndeksi YÃ¼klendi")

        vector_stores[topic] = vectorstore

    return vector_stores

# --- C. RAG SÄ°STEMÄ°NÄ°N BAÅLATILMASI ---

def initialize_rag_system():
    """
    RAG sistemini baÅŸlatÄ±r, VektÃ¶r DepolarÄ±nÄ± yÃ¼kler ve LLM'i hazÄ±rlar.
    Bu, app.py tarafÄ±ndan Ã§aÄŸrÄ±lÄ±r.
    """
    print("\nğŸš€ Ã‡ok KatmanlÄ± RAG Sistemi BaÅŸlatÄ±lÄ±yor...\n")

    # AdÄ±m 1: VektÃ¶r Ä°ndekslerini Kurma/YÃ¼kleme
    vector_stores = setup_vector_stores()

    if not vector_stores:
        raise Exception("HiÃ§bir vektÃ¶r deposu oluÅŸturulamadÄ±!")

    # AdÄ±m 2: LLM'i HazÄ±rlama (Gemini)
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2)

    # VarsayÄ±lan retriever (routing mantÄ±ÄŸÄ± app.py'de yer alÄ±r)
    default_topic = list(vector_stores.keys())[0]
    default_retriever = vector_stores[default_topic].as_retriever(search_kwargs={"k": 4})

    default_rag_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=default_retriever,
        return_source_documents=True  # Kaynak gÃ¶sterimi iÃ§in
    )

    return default_rag_chain, vector_stores, llm


# --- D. UYGULAMA MANTIÄI ---

if __name__ == "__main__":
    try:
        # Ä°ndeksleme iÅŸlemini tetikler
        rag_chain, vector_stores, llm = initialize_rag_system()

        print("\n" + "="*60)
        print("âœ… TÃœM ChromaDB Ä°NDEKSLERÄ° BAÅARIYLA HAZIRLANDI")
        print("="*60)
        print(f"\nHazÄ±r VektÃ¶r DepolarÄ±: {list(vector_stores.keys())}")
        print("\nğŸ’¡ ArtÄ±k Streamlit arayÃ¼zÃ¼nÃ¼ (app.py) Ã§alÄ±ÅŸtÄ±rabilirsiniz!")

        # Test sorgusu (opsiyonel)
        print("\n--- Test Sorgusu ---")
        test_result = rag_chain({"query": "What is scikit-learn?"})
        print(f"Soru: What is scikit-learn?")
        print(f"Cevap: {test_result['result'][:200]}...")

    except Exception as e:
        print(f"\nâŒ KRÄ°TÄ°K HATA: RAG Sistemi baÅŸlatÄ±lamadÄ±.")
        print(f"Hata DetayÄ±: {e}")
        print("\nğŸ” Kontrol Listesi:")
        print("  1. GEMINI_API_KEY .env dosyasÄ±nda tanÄ±mlÄ± mÄ±?")
        print("  2. Sanal ortam (venv) aktif mi?")
        print("  3. Ä°nternet baÄŸlantÄ±nÄ±z Ã§alÄ±ÅŸÄ±yor mu?")
        print("  4. requirements.txt dosyasÄ± kuruldu mu? (pip install -r requirements.txt)")
